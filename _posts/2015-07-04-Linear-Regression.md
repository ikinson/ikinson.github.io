---
layout: post
title: '机器学习-单变量线性回归'
date: 2015-07-04
author: Kinson
categories: 机器学习
tags: 线性回归 梯度下降
---

* content
{:toc}

## 线性回归

线性回归属于监督学习，因此方法和监督学习一致。先给定一个训练集，根据这个训练集学习出一个**线性函数**，然后测试这个函数训练效果（即此函数是否足够拟合训练集数据），根据拟合效果挑选出最好的函数。根据训练集中特征（或输入变量）的数量，线性回归可分为两大类：

- 单变量线性回归

- 多变量线性回归

## 单变量线性回归

### 标记符号

首先定义一下回归问题中的一些标记符号：

- m 代表训练集中实例的数量

- x 代表数据集中特征（或输入）变量数

- y 代表目标变量（或输出变量）

- (x, y) 代表训练集中的实例

- $(x^{(i)},y^{(i)})$  表示第 i 个观察实例

- h 代表学习算法的解决方案或函数，也成为假设（Hypothesis）

### 目标函数

线性回归的目标是根据给定的训练样本集合，训练得到一个线性目标函数h，通过这个函数，对其他未知输入可以预测出正确结果。对于单变量线性回归来说，目标函数表示为：

$$h_\theta(x) = \theta_0 + \theta_1(x).$$

线性回归学习的目标就是要找到最优的参数$\theta_0$和$\theta_1$,使得**建模误差（modeling error）**最小。

### 代价函数

我们应该有一个评价函数来比较预测值和目标值之间的差距，差距越小，目标函数就越准确，我们称之为**代价函数（Cost Function）**。代价函数表示为：

$$J(\theta_0,\theta_1) = \frac{1}{2m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})^2$$

其中$h_\theta(x^{(i)})$为预测值，y^{(i)}为样本的目标值。我们的目标就是**最小化**代价函数：

$$\mathop {minimize }\limits_{\theta_0,\theta_1} J(\theta_0,\theta_1)$$

## 梯度下降法

我们用梯度下降法来求解代价函数最小化问题，梯度下降法的思想为一开始随机选择初始的参数$(\theta_0,\theta_1...\theta_j)$，计算此时的代价函数；然后寻找下一个能让代价函数下降的最多的参数组合，一直这样迭代下去直至找到局部最小值，不能确定该局部最小值是否为全局最小值。

批量梯度下降法(batch gradient decent)的公式为：

$$\theta_j := \theta_j - \alpha\frac{\partial}{\partial\theta_j}J(\theta_0,\theta_1), (j=0,1)$$

注意：各个参数同时更新。对于单变量线性回归而言，参数同时更新迭代步骤为：

$$temp_0 = \theta_0 - \alpha\frac{\partial}{\partial\theta_0}J(\theta_0,\theta_1)$$

$$temp_1 = \theta_1 - \alpha\frac{\partial}{\partial\theta_1}J(\theta_0,\theta_1)$$

$$\theta_0 = temp_0, \theta_1 = temp_1$$

其中$\alpha$为**学习速率（learning rate）**，表示迭代时参数变化的大小。由于梯度越来越小，所以固定的学习速率也能得到局部最优解。

## 代价函数求解

结合梯度下降法，求解代价函数最小值，进行一下迭代直至目标函数收敛：

$$\theta_0 := \theta_0 - \alpha\frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})$$

$$\theta_1 := \theta_1 - \alpha\frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)}).x^{(i)}$$
